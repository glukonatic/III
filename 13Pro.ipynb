{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "13Pro",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/glukonatic/III/blob/master/13Pro.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3j1Wpkvc3Q2s",
        "cellView": "form"
      },
      "source": [
        "#@title Импорт библиотек\n",
        "\n",
        "import numpy as np \n",
        "\n",
        "import re \n",
        "\n",
        "from tensorflow.keras.models import Model, load_model \n",
        "from tensorflow.keras.layers import Dense, Embedding, LSTM, Input \n",
        "from tensorflow.keras.optimizers import RMSprop, Adadelta \n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences \n",
        "from tensorflow.keras.preprocessing.text import Tokenizer \n",
        "from tensorflow.keras import utils \n",
        "from tensorflow.keras.utils import plot_model \n",
        "\n",
        "import yaml "
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "200dSPOYZE7N",
        "cellView": "form",
        "outputId": "9e03a4e4-ee3f-4352-aac7-28cb0a7639d5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#@title Подключаем гуглдрайв\n",
        "\n",
        "from google.colab import files \n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hBazpYMcvOsr",
        "cellView": "both"
      },
      "source": [
        "#@title Класс для ДЗ\n",
        "\n",
        "class HW13():\n",
        "  \"\"\"\n",
        "  Класс для выполнения ДЗ-13 части PRO варианта 1\n",
        "  \"\"\"\n",
        "  \n",
        "  def __init__(self):\n",
        "    \"\"\"\n",
        "    Инициализация класса: переменные класса\n",
        "    \"\"\"\n",
        "    corpuspath = '/content/drive/My Drive/III/13/Диалоги(рассказы).yml'\n",
        "    corpus = open(corpuspath, 'r') \n",
        "    document = yaml.safe_load(corpus)\n",
        "\n",
        "    # Переменная для корпуса вопросов\n",
        "    self.conversations = document['разговоры']\n",
        "    # Список вопросов\n",
        "    self.questions = list()\n",
        "    # Список ответов\n",
        "    self.answers = list() \n",
        "    # Результаты автоматического диалога\n",
        "    self.results = list()\n",
        "\n",
        "    print('Класс инициализирован.')\n",
        "\n",
        "\n",
        "  def prepare(self, tknzr_type = 'simple'):\n",
        "    \"\"\"\n",
        "    Подготовка данных для реализации НС чат-бота\n",
        "    Функция принимает переменную tknzr_type - тип подготовки данных:\n",
        "    tknzr_type = 'with_punctuation': данные обрабатываются с учетом пунктуации,\n",
        "    в любом другом случае выполняется обработка данных без учета пунктуации.\n",
        "    Значение переменной по умолчанию задается равным 'simple'\n",
        "    \"\"\"\n",
        "\n",
        "    self.tokenizer = Tokenizer()\n",
        "    # Для каждого диалога из файла\n",
        "    for con in self.conversations:\n",
        "      # Проверяем, является ли тип ответа строкой\n",
        "      if type(con[1]) == str:\n",
        "        # Добавляем вопрос\n",
        "        self.questions.append(con[0]) \n",
        "        # Добавляем ответ с тегами начала и конца, и если ответов несколько - объединение\n",
        "        self.answers.append('<START> ' + (' ').join(con[1:]) + ' <END>') \n",
        "\n",
        "    # Если передано условие обработки данных, учитывая пунктуацию,\n",
        "    # то выполняется отделение определенных знаков препинания \n",
        "    # для обработки токенайзером, как и слов\n",
        "    if tknzr_type == 'with_punctuation':\n",
        "      # Какие знаки препинания требуется токенизировать как слова:\n",
        "      to_tokenize = '.,!?'\n",
        "      # Анонимная функция для вычленения из текста знаков препинания\n",
        "      re_fun = lambda l: re.sub(r'(['+to_tokenize+'])', r' \\1 ', str(l))\n",
        "      # Обработка вопросов и ответов анонимной функцией\n",
        "      self.questions = list(map(re_fun, self.questions))\n",
        "      self.answers = list(map(re_fun, self.answers))\n",
        "\n",
        "    # Обучаем токенайзер на всех вопросах и ответах\n",
        "    self.tokenizer.fit_on_texts(self.questions + self.answers) \n",
        "    # Получаем словарь токенайзера\n",
        "    vocabularyItems = list(self.tokenizer.word_index.items()) \n",
        "    # Определяем длину словаря токенайзера\n",
        "    self.vocabularySize = len(vocabularyItems)+1 \n",
        "\n",
        "    # Преобразуем вопросы в последовательности целых чисел\n",
        "    tokenizedQuestions = self.tokenizer.texts_to_sequences(self.questions) \n",
        "    # Определяем максимальную длину вопроса\n",
        "    self.maxLenQuestions = max([ len(x) for x in tokenizedQuestions]) \n",
        "    # Приводим токенизированные вопросы к унифицированной форме:\n",
        "    #  равняем длину по максимально длинному вопросу\n",
        "    paddedQuestions = pad_sequences(tokenizedQuestions, \n",
        "                                    maxlen=self.maxLenQuestions, padding='post')\n",
        "    # Преобразуем в numpy-массив полученные токенизированные вопросы\n",
        "    #  для подачи на вход енкодера\n",
        "    self.encoderForInput = np.array(paddedQuestions) \n",
        "\n",
        "    # Преобразуем ответы в последовательности целых чисел\n",
        "    tokenizedAnswers = self.tokenizer.texts_to_sequences(self.answers) \n",
        "    # Определяем максимальную длину ответа\n",
        "    self.maxLenAnswers = max([len(x) for x in tokenizedAnswers]) \n",
        "    # Приводим токенизированные ответы к унифицированной форме:\n",
        "    #  равняем длину по максимально длинному ответу\n",
        "    paddedAnswers = pad_sequences(tokenizedAnswers, \n",
        "                                  maxlen=self.maxLenAnswers, padding='post')\n",
        "    # Преобразуем в numpy-массив полученные токенизированные ответы\n",
        "    #  для подачи на вход декодера\n",
        "    self.decoderForInput = np.array(paddedAnswers) \n",
        "\n",
        "    # Для каждого токенизированного и выровнянного по длине ответа\n",
        "    #  устраняем первый тег <START>\n",
        "    for i in range(len(tokenizedAnswers)) : \n",
        "      tokenizedAnswers[i] = tokenizedAnswers[i][1:] \n",
        "    # Приводим токенизированные усечённые ответы к унифицированной форме:\n",
        "    #  равняем длину по максимально длинному ответу\n",
        "    paddedAnswers = pad_sequences(tokenizedAnswers, \n",
        "                                  maxlen=self.maxLenAnswers , padding='post')\n",
        "    # Преобразуем ответы в one-hot-тензоры\n",
        "    self.oneHotAnswers = utils.to_categorical(paddedAnswers, self.vocabularySize) \n",
        "    # Преобразуем полученные one-hot-тензоры в numpy-массив\n",
        "    #  для подачи на выход декодера\n",
        "    self.decoderForOutput = np.array(self.oneHotAnswers) \n",
        "\n",
        "    print('Данные подготовлены.')\n",
        "  \n",
        "\n",
        "  def create_nn(self):\n",
        "    \"\"\"\n",
        "    Создание нейронной сети чат-бота\n",
        "    \"\"\"\n",
        "\n",
        "    # Вход энкодера\n",
        "    self.encoderInputs = Input(shape=(11, )) \n",
        "    # Добавляем Embedding-слой\n",
        "    encoderEmbedding = Embedding(self.vocabularySize, \n",
        "                                 200,  mask_zero=True) (self.encoderInputs)\n",
        "    # Добавляем слой LSTM с возвратом состояний\n",
        "    _, state_h , state_c = LSTM(200, return_state=True)(encoderEmbedding)\n",
        "    # Получаем список состояний экнодера\n",
        "    self.encoderStates = [state_h, state_c]\n",
        "\n",
        "    # Вход декодера\n",
        "    self.decoderInputs = Input(shape=(13, )) \n",
        "    # Добавляем Embedding-слой \n",
        "    self.decoderEmbedding = Embedding(self.vocabularySize, \n",
        "                                      200, mask_zero=True) (self.decoderInputs) \n",
        "    # Добавляем слой LSTM с возвратом состояний\n",
        "    self.decoderLSTM = LSTM(200, return_state=True, return_sequences=True)\n",
        "    # Получаем выход LSTM-слоя, без сохранения состояний\n",
        "    decoderOutputs , _ , _ = self.decoderLSTM (self.decoderEmbedding, \n",
        "                                               initial_state=self.encoderStates)\n",
        "    # Добавляем полносвязный слой с активацией softmax\n",
        "    self.decoderDense = Dense(self.vocabularySize, activation='softmax') \n",
        "    # Получаем выход декодера\n",
        "    output = self.decoderDense(decoderOutputs)\n",
        "\n",
        "    # Объявляем модель НС чат-бота\n",
        "    self.model = Model([self.encoderInputs, self.decoderInputs], output)\n",
        "    # Компилируем её\n",
        "    self.model.compile(optimizer=RMSprop(), loss='categorical_crossentropy')\n",
        "\n",
        "    print('Модель скомпилирована.')\n",
        "    \n",
        "\n",
        "  def fit(self, bs=50, ep=10, v=0):\n",
        "    \"\"\"\n",
        "    Обучение нейронной сети\n",
        "    \"\"\"\n",
        "\n",
        "    print('Запуск обучения...') \n",
        "    # Непосредственно запуск обучения\n",
        "    self.model.fit([self.encoderForInput , self.decoderForInput], \n",
        "                   self.decoderForOutput, \n",
        "                   batch_size=bs, epochs=ep, verbose = v) \n",
        "    \n",
        "    print('...Цикл обучения завершен')\n",
        "\n",
        "\n",
        "  def makeInferenceModels(self):\n",
        "    \"\"\"\n",
        "    Определение интерференции новой фразы (новый диалог) и \n",
        "      предобученной модели.\n",
        "    В функции объявляются модели энкодера и декодера\n",
        "      для кодирования входной фразы и раскодирования ответа на нее\n",
        "      на основании состояний имеющейся предобученной модели чат-бота\n",
        "    Возвращает модель энкодера и декодера\n",
        "    \"\"\"\n",
        "\n",
        "    # Объявляем модель энкодера для диалога,\n",
        "    #  указываем на входе Input энкодера \n",
        "    #  и тензоры состояния на входе обученной модели\n",
        "    encoderModel = Model(self.encoderInputs, self.encoderStates) \n",
        "\n",
        "    # Объявляем входы декодера\n",
        "    decoderStateInput_h = Input(shape=(200 ,)) \n",
        "    decoderStateInput_c = Input(shape=(200 ,))\n",
        "    \n",
        "    # Описываем переменную, которая будет частью входа декодера\n",
        "    decoderStatesInputs = [decoderStateInput_h, decoderStateInput_c] \n",
        "\n",
        "    # Определяем выходы декодера как состояние узлов LSTM-слоя,\n",
        "    #  описанной для модели чат-бота ранее, \n",
        "    #  при передаче той же матрицы, что и при объявлении обученной модели НС,\n",
        "    #  при этом инициализируем исходное состояние тензорами текущего входа (фразы)\n",
        "    decoderOutputs, state_h, state_c = self.decoderLSTM(self.decoderEmbedding, \n",
        "                                                        initial_state=decoderStatesInputs)\n",
        "\n",
        "    # Описываем переменную, хранящую состояние на выходе LSTM-слоя выше\n",
        "    decoderStates = [state_h, state_c] \n",
        "\n",
        "    # Получаем выход для передачи в модель декодера\n",
        "    decoderOutputs = self.decoderDense(decoderOutputs) \n",
        "    \n",
        "    # Объявляем модель декодера для диалога\n",
        "    #  на входе сумма тензоров входа обученной модели с тензорами текущего входа (диалога),\n",
        "    #  а так же выход, полученный в результате обработки LSTM-слоя объявленной модели\n",
        "    #  при инициализации исходного состояния тензорами текущего входа (фразы)\n",
        "    #  в сумме с состоянием LSTM-модели после подачи диалоговой фразы\n",
        "    decoderModel = Model([self.decoderInputs] + decoderStatesInputs, \n",
        "                         [decoderOutputs] + decoderStates)\n",
        "\n",
        "    # Возвращаем модель энкодера и декодера\n",
        "    return encoderModel , decoderModel\n",
        "\n",
        "\n",
        "  def strToTokens(self, sentence: str): \n",
        "    \"\"\"\n",
        "    Преобразование строки в токен на основании предобученного токенайзера\n",
        "    Входная строка преобразуется в нижний регистр,\n",
        "      и для каждого слова получаем токенизированный тензор,\n",
        "      который так же приводится к унифицированной длине (максимальная длина вопроса)\n",
        "    Возвращает предопределенной величины токенизированный тензор вопроса\n",
        "    \"\"\"\n",
        "\n",
        "    # Строка преобразуется в нижний регистр\n",
        "    words = sentence.lower().split() \n",
        "    # Объявляется список результирующих тензоров\n",
        "    tokensList = list() \n",
        "\n",
        "    # Для каждого слова в вопросе получаем тензор \n",
        "    #  на основании предобученного токенайзера\n",
        "    for word in words: \n",
        "      tokensList.append(self.tokenizer.word_index[word]) if word in self.tokenizer.word_docs.keys() else tokensList.append(1) \n",
        "\n",
        "    # Возвращаем тензоры слов вопроса универсальной длины\n",
        "    return pad_sequences([tokensList], maxlen=self.maxLenQuestions , padding='post')\n",
        "\n",
        "\n",
        "  def main(self, dialog_type = 'manual', phrases = 6):\n",
        "    \"\"\"\n",
        "    Главная функция для запуска чат-бота\n",
        "    \"\"\"\n",
        "\n",
        "    # Берем описанные модели энкодера и декодера для запуска чат-бота\n",
        "    encModel, decModel = self.makeInferenceModels() \n",
        "\n",
        "    # Фразы для автоматического прогона чат-бота\n",
        "    self.auto_phrases = ['Привет','Как дела','Почему так','Ты вообще кто','А если найду','Ужас какой']\n",
        "\n",
        "    # Начинаем диалог\n",
        "    for i in range(phrases): \n",
        "\n",
        "      # Режим общения: автоматический или общение с оператором (пользователем)\n",
        "      #  В случае автоматического - передаем заготовленные фразы по циклу,\n",
        "      #  а при общении с пользователем - приглашаем ввести вопрос в консоли\n",
        "      if dialog_type == 'auto':\n",
        "        statesValues = encModel.predict(self.strToTokens(self.auto_phrases[i]))\n",
        "      else:\n",
        "        statesValues = encModel.predict(self.strToTokens(input( 'Задайте вопрос : ' )))\n",
        "\n",
        "      # Объявляем матрицу (1,1) и записываем в ячейку (0,0) индекс тега 'start'\n",
        "      emptyTargetSeq = np.zeros((1, 1))    \n",
        "      emptyTargetSeq[0, 0] = self.tokenizer.word_index['start'] \n",
        "\n",
        "      # Инициализируем условие остановки цикла определения ответа\n",
        "      stopCondition = False \n",
        "\n",
        "      # Декодированный ответ чат-бота\n",
        "      decodedTranslation = '' \n",
        "\n",
        "      # Бесконечный цикл, пока не заявлено стоп-условие\n",
        "      while not stopCondition : \n",
        "        \n",
        "        # Моделью декодера предсказываем ответ (по-словно),\n",
        "        #  получаем тензор-ответ и состояние модели бота при получении фразы\n",
        "        decOutputs , h , c = decModel.predict([emptyTargetSeq] + statesValues)\n",
        "      \n",
        "        sampledWordIndex = np.argmax(decOutputs, axis=-1) \n",
        "        sampledWord = None \n",
        "        for word , index in self.tokenizer.word_index.items():\n",
        "          if sampledWordIndex == index: \n",
        "            decodedTranslation += ' {}'.format(word) \n",
        "            sampledWord = word \n",
        "      \n",
        "        if sampledWord == 'end' or len(decodedTranslation.split()) > self.maxLenAnswers:\n",
        "          stopCondition = True \n",
        "\n",
        "        emptyTargetSeq = np.zeros((1, 1)) \n",
        "        emptyTargetSeq[0, 0] = sampledWordIndex \n",
        "        statesValues = [h, c] \n",
        "\n",
        "      if dialog_type == 'auto':\n",
        "        self.results.append(decodedTranslation[:-3]) \n",
        "      else:\n",
        "        print(decodedTranslation[:-3]) \n",
        "\n",
        "\n",
        "  def clear_vars(self):\n",
        "    \"\"\"\n",
        "    Очищение памяти виртуального окружения от переменных класса\n",
        "    \"\"\"\n",
        "\n",
        "    self.questions.clear()\n",
        "    self.answers.clear()\n",
        "    del self.tokenizer\n",
        "    del self.vocabularySize\n",
        "    del self.maxLenQuestions\n",
        "    del self.encoderForInput\n",
        "    del self.maxLenAnswers\n",
        "    del self.decoderForInput\n",
        "    del self.oneHotAnswers\n",
        "    del self.decoderForOutput\n",
        "\n",
        "    print('Переменные удалены и очищены')"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1AX70SzBALgb",
        "outputId": "506d2d54-44d2-4ab1-fdf2-c462cf8b1e92",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(np.zeros((1, 1)))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fMavUuzC7IlQ",
        "cellView": "form",
        "outputId": "422e42a3-1519-41ee-d1d8-b1d1c107bc03",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#@title Главный блок\n",
        "\n",
        "#@markdown Количество циклов обучения по 10 эпох:\n",
        "cycles =  10#@param {type: \"number\"}\n",
        "\n",
        "first = HW13()\n",
        "\n",
        "first.prepare()\n",
        "first.create_nn()\n",
        "\n",
        "for i in range(cycles):\n",
        "  first.fit()\n",
        "  first.main('auto')\n",
        "\n",
        "first.clear_vars()\n",
        "\n",
        "second = HW13()\n",
        "\n",
        "second.prepare('with_punctuation')\n",
        "second.create_nn()\n",
        "\n",
        "for i in range(cycles):\n",
        "  second.fit()\n",
        "  second.main('auto')\n",
        "\n",
        "second.clear_vars()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Класс инициализирован.\n",
            "Данные подготовлены.\n",
            "Модель скомпилирована.\n",
            "Запуск обучения...\n",
            "Цикл обучения завершен\n",
            "WARNING:tensorflow:Model was constructed with shape (None, 13) for input Tensor(\"input_2:0\", shape=(None, 13), dtype=float32), but it was called on an input with incompatible shape (None, 1).\n",
            "Запуск обучения...\n",
            "Цикл обучения завершен\n",
            "WARNING:tensorflow:Model was constructed with shape (None, 13) for input Tensor(\"input_2:0\", shape=(None, 13), dtype=float32), but it was called on an input with incompatible shape (None, 1).\n",
            "Запуск обучения...\n",
            "Цикл обучения завершен\n",
            "WARNING:tensorflow:Model was constructed with shape (None, 13) for input Tensor(\"input_2:0\", shape=(None, 13), dtype=float32), but it was called on an input with incompatible shape (None, 1).\n",
            "Запуск обучения...\n",
            "Цикл обучения завершен\n",
            "WARNING:tensorflow:Model was constructed with shape (None, 13) for input Tensor(\"input_2:0\", shape=(None, 13), dtype=float32), but it was called on an input with incompatible shape (None, 1).\n",
            "Запуск обучения...\n",
            "Цикл обучения завершен\n",
            "WARNING:tensorflow:Model was constructed with shape (None, 13) for input Tensor(\"input_2:0\", shape=(None, 13), dtype=float32), but it was called on an input with incompatible shape (None, 1).\n",
            "Запуск обучения...\n",
            "Цикл обучения завершен\n",
            "WARNING:tensorflow:Model was constructed with shape (None, 13) for input Tensor(\"input_2:0\", shape=(None, 13), dtype=float32), but it was called on an input with incompatible shape (None, 1).\n",
            "Запуск обучения...\n",
            "Цикл обучения завершен\n",
            "WARNING:tensorflow:Model was constructed with shape (None, 13) for input Tensor(\"input_2:0\", shape=(None, 13), dtype=float32), but it was called on an input with incompatible shape (None, 1).\n",
            "Запуск обучения...\n",
            "Цикл обучения завершен\n",
            "WARNING:tensorflow:Model was constructed with shape (None, 13) for input Tensor(\"input_2:0\", shape=(None, 13), dtype=float32), but it was called on an input with incompatible shape (None, 1).\n",
            "Запуск обучения...\n",
            "Цикл обучения завершен\n",
            "WARNING:tensorflow:Model was constructed with shape (None, 13) for input Tensor(\"input_2:0\", shape=(None, 13), dtype=float32), but it was called on an input with incompatible shape (None, 1).\n",
            "Запуск обучения...\n",
            "Цикл обучения завершен\n",
            "WARNING:tensorflow:Model was constructed with shape (None, 13) for input Tensor(\"input_2:0\", shape=(None, 13), dtype=float32), but it was called on an input with incompatible shape (None, 1).\n",
            "Переменные удалены и очищены\n",
            "Класс инициализирован.\n",
            "Данные подготовлены.\n",
            "Модель скомпилирована.\n",
            "Запуск обучения...\n",
            "Цикл обучения завершен\n",
            "WARNING:tensorflow:Model was constructed with shape (None, 13) for input Tensor(\"input_24:0\", shape=(None, 13), dtype=float32), but it was called on an input with incompatible shape (None, 1).\n",
            "Запуск обучения...\n",
            "Цикл обучения завершен\n",
            "WARNING:tensorflow:Model was constructed with shape (None, 13) for input Tensor(\"input_24:0\", shape=(None, 13), dtype=float32), but it was called on an input with incompatible shape (None, 1).\n",
            "Запуск обучения...\n",
            "Цикл обучения завершен\n",
            "WARNING:tensorflow:Model was constructed with shape (None, 13) for input Tensor(\"input_24:0\", shape=(None, 13), dtype=float32), but it was called on an input with incompatible shape (None, 1).\n",
            "Запуск обучения...\n",
            "Цикл обучения завершен\n",
            "WARNING:tensorflow:Model was constructed with shape (None, 13) for input Tensor(\"input_24:0\", shape=(None, 13), dtype=float32), but it was called on an input with incompatible shape (None, 1).\n",
            "Запуск обучения...\n",
            "Цикл обучения завершен\n",
            "WARNING:tensorflow:Model was constructed with shape (None, 13) for input Tensor(\"input_24:0\", shape=(None, 13), dtype=float32), but it was called on an input with incompatible shape (None, 1).\n",
            "Запуск обучения...\n",
            "Цикл обучения завершен\n",
            "WARNING:tensorflow:Model was constructed with shape (None, 13) for input Tensor(\"input_24:0\", shape=(None, 13), dtype=float32), but it was called on an input with incompatible shape (None, 1).\n",
            "Запуск обучения...\n",
            "Цикл обучения завершен\n",
            "WARNING:tensorflow:Model was constructed with shape (None, 13) for input Tensor(\"input_24:0\", shape=(None, 13), dtype=float32), but it was called on an input with incompatible shape (None, 1).\n",
            "Запуск обучения...\n",
            "Цикл обучения завершен\n",
            "WARNING:tensorflow:Model was constructed with shape (None, 13) for input Tensor(\"input_24:0\", shape=(None, 13), dtype=float32), but it was called on an input with incompatible shape (None, 1).\n",
            "Запуск обучения...\n",
            "Цикл обучения завершен\n",
            "WARNING:tensorflow:Model was constructed with shape (None, 13) for input Tensor(\"input_24:0\", shape=(None, 13), dtype=float32), but it was called on an input with incompatible shape (None, 1).\n",
            "Запуск обучения...\n",
            "Цикл обучения завершен\n",
            "WARNING:tensorflow:Model was constructed with shape (None, 13) for input Tensor(\"input_24:0\", shape=(None, 13), dtype=float32), but it was called on an input with incompatible shape (None, 1).\n",
            "Переменные удалены и очищены\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DLB4gPZO7XP3",
        "outputId": "ed5d3bb1-1862-44fe-cf19-84ab4de4d947",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "ттттттттттттprint(first.results)\n",
        "print(second.results)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[' да ', ' да ', ' не знаю ', ' нет нет ', ' да ', ' да ', ' что случилось ', ' да товарищ командир ', ' не знаю ', ' нет я не знаю а что ', ' как же вы знаете ', ' а где ', ' ну как что вы с вами делать ', ' а почему бы и нет ', ' потому что у меня есть есть ', ' я не знаю я а ты мне не об этом этом ', ' как мы сами и с тобой и я ', ' а где ', ' что ты здесь делаешь ', ' все в порядке ', ' потому что у вас там нет ', ' меня я должен знать сейчас пойду ты ', ' как мы с вами все и в порядке ', ' ну на что нибудь ', ' что ты здесь делаешь ', ' вот так ', ' ты так просто ', ' меня я пойду один сейчас ты меня не умею меня ', ' ну и сказала все и его к нет ', ' на кто нибудь из чего ', ' что ты здесь делаешь ', ' за вами генерал ', ' потому что у вас есть они ', ' а я конечно не сколько ', ' ну и сказала я тоже не знаю ', ' ну на что нибудь ', ' что тебе надо человек ', ' за вами ну вот так ', ' потому что у вас есть ', ' а я конечно не сказала что ли кто не его в дело ', ' мы будем из тебя ', ' на я тебя могу ему еще спасибо будем ', ' что ты здесь делаешь ', ' за все теперь ты о том у меня зовут ', ' потому что у вас все вот так знаешь ', ' а я хочу что это сделал ', ' мы будем об этом ', ' ну такой ', ' что ты здесь делаешь ', ' за все теперь о ', ' потому что у вас там ничего нет ', ' я не а мне не нравится ', ' мы будем об этом ', ' ну такой ', ' что ты здесь делаешь ', ' за ночь сделали пятнадцать у тебя ', ' потому что у меня там ничего нет ', ' а я почему не знаю ', ' мы будем да по об этом ', ' ну такой ']\n",
            "[' да ', ' да ', ' ничего ', ' нет нет ', ' да ', ' ну да ', ' что вы знаете ', ' да вот ', ' ничего ', ' я я знаю я тебя не знаю ', ' как именно ', ' а вот кто ', ' что вы знаете ', ' да все в порядке ', ' мне так ', ' я я не знаю ну ка мне ', ' как же вы кто их ', ' на каком смысле ', ' что тебе надо человек ', ' да вот так уж не сейчас ', ' не хочу ', ' я я не могу я ничего с тобой ', ' как же вы если если в быть я ', ' ну вот так ', ' что ты здесь делаешь ', ' вот вам о чем только было от меня ', ' не хочу ', ' я я в ну ж ж ничего ж ', ' как хочешь ', ' ну да ', ' что ты здесь делаешь ', ' за тобой все в порядке ', ' ну ', ' я я не могу в на ну ', ' как тогда я ', ' с вами хочет чтобы тоже ', ' что ты здесь делаешь ', ' за тобой все ', ' ну ', ' я я в ну что ж и ничего а ', ' а вы кто же теперь ', ' с вами хочет чтобы тоже ', ' что тебе надо человек ', ' за вообще тоже кое что надо надо там ', ' ну ', ' я я в ну чтобы все ему ну ', ' а как ', ' с вами ', ' что тебе надо человек ', ' за десять минут вот ', ' да так ', ' я я в ну что ж нет то что ', ' а я все это ', ' с а что здесь ничего ', ' что ты здесь делаешь ', ' за десять минут пятнадцать ', ' да так я все мы с порядке ', ' я я в ну что то ж хотел ', ' а я как его не это теперь ', ' с это да ']\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
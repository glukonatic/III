{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "13Ultra",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/glukonatic/III/blob/master/13Ultra.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3j1Wpkvc3Q2s",
        "cellView": "form"
      },
      "source": [
        "#@title Импорт библиотек\n",
        "\n",
        "import numpy as np \n",
        "\n",
        "import re \n",
        "\n",
        "from tensorflow.keras.models import Model, load_model \n",
        "from tensorflow.keras.layers import Dense, Embedding, LSTM, Input \n",
        "from tensorflow.keras.optimizers import RMSprop, Adadelta \n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences \n",
        "from tensorflow.keras.preprocessing.text import Tokenizer \n",
        "from tensorflow.keras import utils \n",
        "from tensorflow.keras.utils import plot_model \n",
        "\n",
        "import yaml "
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "200dSPOYZE7N",
        "cellView": "form",
        "outputId": "8de92d9a-940a-4dcc-c7a1-eff7d54211a1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#@title Подключаем гуглдрайв\n",
        "\n",
        "from google.colab import files \n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hBazpYMcvOsr",
        "cellView": "both"
      },
      "source": [
        "#@title Класс для ДЗ\n",
        "\n",
        "class HW13():\n",
        "  \"\"\"\n",
        "  Класс для выполнения ДЗ-13 части PRO варианта 1\n",
        "  \"\"\"\n",
        "  \n",
        "  def __init__(self):\n",
        "    \"\"\"\n",
        "    Инициализация класса: переменные класса\n",
        "    \"\"\"\n",
        "    corpuspath = '/content/drive/My Drive/answer_database.txt'\n",
        "    # corpuspath = '/content/drive/My Drive/III/13/Диалоги(рассказы).yml'\n",
        "    corpus = open(corpuspath, 'r', encoding='utf-8') \n",
        "    # document = yaml.safe_load(corpus)\n",
        "\n",
        "    # Переменная для корпуса вопросов\n",
        "    # self.conversations = document['разговоры']\n",
        "    self.conversations = list()\n",
        "    for line in corpus:\n",
        "      self.conversations.append(line)\n",
        "    corpus.close()\n",
        "    # Список вопросов\n",
        "    self.questions = list()\n",
        "    # Список ответов\n",
        "    self.answers = list() \n",
        "    # Результаты автоматического диалога\n",
        "    self.results = list()\n",
        "\n",
        "    print('Класс инициализирован.')\n",
        "\n",
        "\n",
        "  def prepare(self, tknzr_type = 'simple'):\n",
        "    \"\"\"\n",
        "    Подготовка данных для реализации НС чат-бота\n",
        "    Функция принимает переменную tknzr_type - тип подготовки данных:\n",
        "    tknzr_type = 'with_punctuation': данные обрабатываются с учетом пунктуации,\n",
        "    в любом другом случае выполняется обработка данных без учета пунктуации.\n",
        "    Значение переменной по умолчанию задается равным 'simple'\n",
        "    \"\"\"\n",
        "\n",
        "    self.tokenizer = Tokenizer()\n",
        "\n",
        "    # Для каждого диалога из файла\n",
        "    for c in self.conversations:\n",
        "      # Вычленяем вопрос и ответ\n",
        "      con = re.search(r'^([\\w\\s.,?!\\-\\'\\\";:\\)]+)\\\\([\\w\\s,\\*.\\'\\\";:\\)\\(\\[\\]!?\\-]+).+$',c.strip(u'\\ufeff'))\n",
        "      # Если есть найденные по шаблону вопрос и ответ\n",
        "      if con and len(con.groups()) == 2:\n",
        "        # Добавляем вопрос\n",
        "        self.questions.append(con.group(1))\n",
        "        # Добавляем ответ с тегами начала и конца, и если ответов несколько - объединение\n",
        "        self.answers.append('<START> ' + con.group(2) + ' <END>') \n",
        "      # Иначе - переходим к следующей строке вопрос-ответ\n",
        "      else:\n",
        "        continue\n",
        "\n",
        "    print('Обработаны {} вопросов и {} ответов к ним'.format(len(self.questions),len(self.answers)))\n",
        "\n",
        "    # Если передано условие обработки данных, учитывая пунктуацию,\n",
        "    # то выполняется отделение определенных знаков препинания \n",
        "    # для обработки токенайзером, как и слов\n",
        "    if tknzr_type == 'with_punctuation':\n",
        "      # Какие знаки препинания требуется токенизировать как слова:\n",
        "      to_tokenize = '.,!?'\n",
        "      # Анонимная функция для вычленения из текста знаков препинания\n",
        "      re_fun = lambda l: re.sub(r'(['+to_tokenize+'])', r' \\1 ', str(l))\n",
        "      # Обработка вопросов и ответов анонимной функцией\n",
        "      self.questions = list(map(re_fun, self.questions))\n",
        "      self.answers = list(map(re_fun, self.answers))\n",
        "      \n",
        "      print('Выполнена предобработка для версии, учитывающей пунктуацию')\n",
        "\n",
        "    # Обучаем токенайзер на всех вопросах и ответах\n",
        "    self.tokenizer.fit_on_texts(self.questions + self.answers) \n",
        "    # Получаем словарь токенайзера\n",
        "    vocabularyItems = list(self.tokenizer.word_index.items()) \n",
        "    # Определяем длину словаря токенайзера\n",
        "    self.vocabularySize = len(vocabularyItems)+1 \n",
        "\n",
        "    print('Токенайзер обучен. В словаре {} слов'.format(self.vocabularySize))\n",
        "\n",
        "    # Преобразуем вопросы в последовательности целых чисел\n",
        "    tokenizedQuestions = self.tokenizer.texts_to_sequences(self.questions) \n",
        "    # Определяем максимальную длину вопроса\n",
        "    self.maxLenQuestions = max([ len(x) for x in tokenizedQuestions]) \n",
        "    # Приводим токенизированные вопросы к унифицированной форме:\n",
        "    #  равняем длину по максимально длинному вопросу\n",
        "    paddedQuestions = pad_sequences(tokenizedQuestions, \n",
        "                                    maxlen=self.maxLenQuestions, padding='post')\n",
        "    # Преобразуем в numpy-массив полученные токенизированные вопросы\n",
        "    #  для подачи на вход енкодера\n",
        "    self.encoderForInput = np.array(paddedQuestions) \n",
        "\n",
        "    print('Размерность данных на входе энкодера {}'.format(self.encoderForInput.shape))\n",
        "\n",
        "    # Преобразуем ответы в последовательности целых чисел\n",
        "    tokenizedAnswers = self.tokenizer.texts_to_sequences(self.answers) \n",
        "    # Определяем максимальную длину ответа\n",
        "    self.maxLenAnswers = max([len(x) for x in tokenizedAnswers]) \n",
        "    # Приводим токенизированные ответы к унифицированной форме:\n",
        "    #  равняем длину по максимально длинному ответу\n",
        "    paddedAnswers = pad_sequences(tokenizedAnswers, \n",
        "                                  maxlen=self.maxLenAnswers, padding='post')\n",
        "    # Преобразуем в numpy-массив полученные токенизированные ответы\n",
        "    #  для подачи на вход декодера\n",
        "    self.decoderForInput = np.array(paddedAnswers) \n",
        "\n",
        "    print('Размерность данных на входе декодера {}'.format(self.decoderForInput.shape))\n",
        "\n",
        "    # Для каждого токенизированного и выровнянного по длине ответа\n",
        "    #  устраняем первый тег <START>\n",
        "    for i in range(len(tokenizedAnswers)) : \n",
        "      tokenizedAnswers[i] = tokenizedAnswers[i][1:] \n",
        "    # Приводим токенизированные усечённые ответы к унифицированной форме:\n",
        "    #  равняем длину по максимально длинному ответу\n",
        "    paddedAnswers = pad_sequences(tokenizedAnswers, \n",
        "                                  maxlen=self.maxLenAnswers , padding='post')\n",
        "    # Преобразуем ответы в one-hot-тензоры\n",
        "    self.oneHotAnswers = utils.to_categorical(paddedAnswers, self.vocabularySize) \n",
        "    # Преобразуем полученные one-hot-тензоры в numpy-массив\n",
        "    #  для подачи на выход декодера\n",
        "    self.decoderForOutput = np.array(self.oneHotAnswers) \n",
        "\n",
        "    print('Размерность one-hot данных на выходе декодера {}'.format(self.decoderForOutput.shape))\n",
        "\n",
        "    print('Данные подготовлены.')\n",
        "  \n",
        "\n",
        "  def create_nn(self):\n",
        "    \"\"\"\n",
        "    Создание нейронной сети чат-бота\n",
        "    \"\"\"\n",
        "\n",
        "    # Вход энкодера\n",
        "    self.encoderInputs = Input(shape=(11, )) \n",
        "    # Добавляем Embedding-слой\n",
        "    encoderEmbedding = Embedding(self.vocabularySize, \n",
        "                                 200,  mask_zero=True) (self.encoderInputs)\n",
        "    # Добавляем слой LSTM с возвратом состояний\n",
        "    _, state_h , state_c = LSTM(200, return_state=True)(encoderEmbedding)\n",
        "    # Получаем список состояний экнодера\n",
        "    self.encoderStates = [state_h, state_c]\n",
        "\n",
        "    # Вход декодера\n",
        "    self.decoderInputs = Input(shape=(13, )) \n",
        "    # Добавляем Embedding-слой \n",
        "    self.decoderEmbedding = Embedding(self.vocabularySize, \n",
        "                                      200, mask_zero=True) (self.decoderInputs) \n",
        "    # Добавляем слой LSTM с возвратом состояний\n",
        "    self.decoderLSTM = LSTM(200, return_state=True, return_sequences=True)\n",
        "    # Получаем выход LSTM-слоя, без сохранения состояний\n",
        "    decoderOutputs , _ , _ = self.decoderLSTM (self.decoderEmbedding, \n",
        "                                               initial_state=self.encoderStates)\n",
        "    # Добавляем полносвязный слой с активацией softmax\n",
        "    self.decoderDense = Dense(self.vocabularySize, activation='softmax') \n",
        "    # Получаем выход декодера\n",
        "    output = self.decoderDense(decoderOutputs)\n",
        "\n",
        "    # Объявляем модель НС чат-бота\n",
        "    self.model = Model([self.encoderInputs, self.decoderInputs], output)\n",
        "    # Компилируем её\n",
        "    self.model.compile(optimizer=RMSprop(), loss='categorical_crossentropy')\n",
        "\n",
        "    print('Модель скомпилирована.')\n",
        "    \n",
        "\n",
        "  def fit(self, bs=50, ep=10, v=0):\n",
        "    \"\"\"\n",
        "    Обучение нейронной сети\n",
        "    \"\"\"\n",
        "\n",
        "    print('Запуск обучения...') \n",
        "    # Непосредственно запуск обучения\n",
        "    self.model.fit([self.encoderForInput , self.decoderForInput], \n",
        "                   self.decoderForOutput, \n",
        "                   batch_size=bs, epochs=ep, verbose = v) \n",
        "    \n",
        "    print('...Цикл обучения завершен')\n",
        "\n",
        "\n",
        "  def makeInferenceModels(self):\n",
        "    \"\"\"\n",
        "    Определение интерференции новой фразы (новый диалог) и \n",
        "      предобученной модели.\n",
        "    В функции объявляются модели энкодера и декодера\n",
        "      для кодирования входной фразы и раскодирования ответа на нее\n",
        "      на основании состояний имеющейся предобученной модели чат-бота\n",
        "    Возвращает модель энкодера и декодера\n",
        "    \"\"\"\n",
        "\n",
        "    # Объявляем модель энкодера для диалога,\n",
        "    #  указываем на входе Input энкодера \n",
        "    #  и тензоры состояния на входе обученной модели\n",
        "    encoderModel = Model(self.encoderInputs, self.encoderStates) \n",
        "\n",
        "    # Объявляем входы декодера\n",
        "    decoderStateInput_h = Input(shape=(200 ,)) \n",
        "    decoderStateInput_c = Input(shape=(200 ,))\n",
        "    \n",
        "    # Описываем переменную, которая будет частью входа декодера\n",
        "    decoderStatesInputs = [decoderStateInput_h, decoderStateInput_c] \n",
        "\n",
        "    # Определяем выходы декодера как состояние узлов LSTM-слоя,\n",
        "    #  описанной для модели чат-бота ранее, \n",
        "    #  при передаче той же матрицы, что и при объявлении обученной модели НС,\n",
        "    #  при этом инициализируем исходное состояние тензорами текущего входа (фразы)\n",
        "    decoderOutputs, state_h, state_c = self.decoderLSTM(self.decoderEmbedding, \n",
        "                                                        initial_state=decoderStatesInputs)\n",
        "\n",
        "    # Описываем переменную, хранящую состояние на выходе LSTM-слоя выше\n",
        "    decoderStates = [state_h, state_c] \n",
        "\n",
        "    # Получаем выход для передачи в модель декодера\n",
        "    decoderOutputs = self.decoderDense(decoderOutputs) \n",
        "    \n",
        "    # Объявляем модель декодера для диалога\n",
        "    #  на входе сумма тензоров входа обученной модели с тензорами текущего входа (диалога),\n",
        "    #  а так же выход, полученный в результате обработки LSTM-слоя объявленной модели\n",
        "    #  при инициализации исходного состояния тензорами текущего входа (фразы)\n",
        "    #  в сумме с состоянием LSTM-модели после подачи диалоговой фразы\n",
        "    decoderModel = Model([self.decoderInputs] + decoderStatesInputs, \n",
        "                         [decoderOutputs] + decoderStates)\n",
        "\n",
        "    # Возвращаем модель энкодера и декодера\n",
        "    return encoderModel , decoderModel\n",
        "\n",
        "\n",
        "  def strToTokens(self, sentence: str): \n",
        "    \"\"\"\n",
        "    Преобразование строки в токен на основании предобученного токенайзера\n",
        "    Входная строка преобразуется в нижний регистр,\n",
        "      и для каждого слова получаем токенизированный тензор,\n",
        "      который так же приводится к унифицированной длине (максимальная длина вопроса)\n",
        "    Возвращает предопределенной величины токенизированный тензор вопроса\n",
        "    \"\"\"\n",
        "\n",
        "    # Строка преобразуется в нижний регистр\n",
        "    words = sentence.lower().split() \n",
        "    # Объявляется список результирующих тензоров\n",
        "    tokensList = list() \n",
        "\n",
        "    # Для каждого слова в вопросе получаем тензор \n",
        "    #  на основании предобученного токенайзера\n",
        "    for word in words: \n",
        "      tokensList.append(self.tokenizer.word_index[word]) if word in self.tokenizer.word_docs.keys() else tokensList.append(1) \n",
        "\n",
        "    # Возвращаем тензоры слов вопроса универсальной длины\n",
        "    return pad_sequences([tokensList], maxlen=self.maxLenQuestions , padding='post')\n",
        "\n",
        "\n",
        "  def main(self, dialog_type = 'manual', phrases = 6):\n",
        "    \"\"\"\n",
        "    Главная функция для запуска чат-бота\n",
        "    \"\"\"\n",
        "\n",
        "    # Берем описанные модели энкодера и декодера для запуска чат-бота\n",
        "    encModel, decModel = self.makeInferenceModels() \n",
        "\n",
        "    # Фразы для автоматического прогона чат-бота\n",
        "    self.auto_phrases = ['Привет','Как дела','Почему так','Ты вообще кто','А если найду','Ужас какой']\n",
        "\n",
        "    # Начинаем диалог\n",
        "    for i in range(phrases): \n",
        "\n",
        "      # Режим общения: автоматический или общение с оператором (пользователем)\n",
        "      #  В случае автоматического - передаем заготовленные фразы по циклу,\n",
        "      #  а при общении с пользователем - приглашаем ввести вопрос в консоли\n",
        "      if dialog_type == 'auto':\n",
        "        statesValues = encModel.predict(self.strToTokens(self.auto_phrases[i]))\n",
        "      else:\n",
        "        statesValues = encModel.predict(self.strToTokens(input( 'Задайте вопрос : ' )))\n",
        "\n",
        "      # Объявляем матрицу (1,1) и записываем в ячейку (0,0) индекс тега 'start'\n",
        "      emptyTargetSeq = np.zeros((1, 1))    \n",
        "      emptyTargetSeq[0, 0] = self.tokenizer.word_index['start'] \n",
        "\n",
        "      # Инициализируем условие остановки цикла определения ответа\n",
        "      stopCondition = False \n",
        "\n",
        "      # Декодированный ответ чат-бота\n",
        "      decodedTranslation = '' \n",
        "\n",
        "      # Бесконечный цикл, пока не заявлено стоп-условие\n",
        "      while not stopCondition : \n",
        "        \n",
        "        # Моделью декодера предсказываем ответ (по-словно),\n",
        "        #  получаем тензор-ответ и состояние модели бота при получении фразы\n",
        "        decOutputs , h , c = decModel.predict([emptyTargetSeq] + statesValues)\n",
        "\n",
        "        # Берем индекс наиболее вероятного слова, которое спредиктил декодер\n",
        "        sampledWordIndex = np.argmax(decOutputs, axis=-1) \n",
        "\n",
        "        # Получаем само слово из его индекса\n",
        "        sampledWord = None \n",
        "        for word , index in self.tokenizer.word_index.items():\n",
        "          if sampledWordIndex == index: \n",
        "            decodedTranslation += ' {}'.format(word) \n",
        "            sampledWord = word \n",
        "        # sampledWord = list(self.tokenizer.word_index.keys())[list(self.tokenizer.word_index.values()).index(sampledWordIndex)]\n",
        "        # if sampledWordIndex in self.tokenizer.word_index.values():\n",
        "        #   sampledWord = list(self.tokenizer.word_index.keys())[list(self.tokenizer.word_index.values()).index(sampledWordIndex)]\n",
        "        # else:\n",
        "        #   sampledWord = None\n",
        "\n",
        "        # Если встретили стоп-слово \n",
        "        #  или сгенерированный ответ больше максимальной длины ответов,\n",
        "        #  то останавливаем бесконечный цикл, установкой стоп-условия\n",
        "        if sampledWord == 'end' or len(decodedTranslation.split()) > self.maxLenAnswers:\n",
        "          stopCondition = True \n",
        "\n",
        "        # Объявляем матрицу (1,1) и записываем в ячейку (0,0) \n",
        "        #  индекс текущего слова ответа\n",
        "        emptyTargetSeq = np.zeros((1, 1)) \n",
        "        emptyTargetSeq[0, 0] = sampledWordIndex \n",
        "\n",
        "        # Запоминаем текущее состояние модели бота\n",
        "        statesValues = [h, c] \n",
        "\n",
        "      # Если режим общения автоматический, то заполняем список результатов\n",
        "      if dialog_type == 'auto':\n",
        "        self.results.append(decodedTranslation[:-3]) \n",
        "        print(decodedTranslation[:-3]) \n",
        "      else:\n",
        "        print(decodedTranslation[:-3]) \n",
        "\n",
        "\n",
        "  def clear_vars(self):\n",
        "    \"\"\"\n",
        "    Очистка памяти виртуального окружения от переменных класса\n",
        "    \"\"\"\n",
        "\n",
        "    # self.questions.clear()\n",
        "    # self.answers.clear()\n",
        "    # del self.tokenizer\n",
        "    # del self.vocabularySize\n",
        "    # del self.maxLenQuestions\n",
        "    # del self.encoderForInput\n",
        "    # del self.maxLenAnswers\n",
        "    # del self.decoderForInput\n",
        "    del self.oneHotAnswers\n",
        "    del self.decoderForOutput\n",
        "\n",
        "    print('Переменные удалены и очищены')"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qDhBj1a69BVM",
        "cellView": "form"
      },
      "source": [
        "#@title Функция вывода диалогов обоих ботов\n",
        "\n",
        "#@markdown Количество циклов обучения по 10 эпох:\n",
        "cycles =  10#@param {type: \"number\"}\n",
        "\n",
        "def new_conv(c = cycles):\n",
        "  for i in range(c):\n",
        "    print('{}-й цикл обучения:'.format(i+1))\n",
        "    for phrase in first.auto_phrases:\n",
        "      print(\"\\tВопрос:\",phrase)\n",
        "      # print(i, len(first.results), first.auto_phrases.index(phrase))\n",
        "      print(\"Первый бот:\",first.results[i*len(first.auto_phrases) + first.auto_phrases.index(phrase)])\n",
        "      # print(\"Второй бот:\",second.results[i*len(second.auto_phrases) + second.auto_phrases.index(phrase)])    \n",
        "      print('\\n')\n",
        "    print('\\n')"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fMavUuzC7IlQ",
        "cellView": "form",
        "outputId": "30b0d8e6-28b8-4879-dad4-c05b5756d049",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#@title Главный блок\n",
        "\n",
        "first = HW13()\n",
        "\n",
        "first.prepare('with_punctuation')\n",
        "first.create_nn()\n",
        "\n",
        "for i in range(cycles):\n",
        "  first.fit(ep=100)\n",
        "  first.main('auto')\n",
        "\n",
        "new_conv()\n",
        "\n",
        "# first.clear_vars()\n",
        "\n",
        "# second = HW13()\n",
        "\n",
        "# second.prepare()\n",
        "# second.create_nn()\n",
        "\n",
        "# for i in range(cycles):\n",
        "#   second.fit()\n",
        "#   second.main('auto')\n",
        "\n",
        "# second.clear_vars()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Класс инициализирован.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jUpv6PmD7pmK",
        "cellView": "form"
      },
      "source": [
        "#@title Функция вывода диалогов обоих ботов\n",
        "\n",
        "new_conv()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}